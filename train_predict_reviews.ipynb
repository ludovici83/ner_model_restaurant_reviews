{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=4\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from datasets import load_metric\n",
    "from transformers import  RobertaTokenizerFast,AutoTokenizer\n",
    "from transformers import RobertaForTokenClassification,AutoModelForTokenClassification\n",
    "from transformers import pipeline, TrainingArguments,Trainer\n",
    "import numpy as np\n",
    "%env CUDA_VISIBLE_DEVICES=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b6c22ba7c928d56f\n",
      "Reusing dataset pandas (/datos/luis/.cache/datasets/pandas/default-b6c22ba7c928d56f/0.0.0/6197c1e855b639d75a767140856841a562b7a71d129104973fe1962594877ade)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3fa580bb484cb5b863bd910b16cdcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_reviews = load_dataset(\"pandas\", data_files = {\"train\": \"df_train.pkl\",\"test\":\"df_test.pkl\",\"validation\":\"df_val.pkl\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_reviews = ds_reviews.remove_columns(\"__index_level_0__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = ds_reviews[\"test\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>la atención maravillosa, imposible pedir más.</td>\n",
       "      <td>[la, atención, maravillosa, ,, imposible, pedi...</td>\n",
       "      <td>[O, O, B-MODIFIER, O, O, O, O, O]</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>salchicha alemana muy grande y el secreto muy ...</td>\n",
       "      <td>[salchicha, alemana, muy, grande, y, el, secre...</td>\n",
       "      <td>[B-CONCEPT, O, B-MODIFIER, I-MODIFIER, O, O, B...</td>\n",
       "      <td>[3, 0, 1, 2, 0, 0, 3, 1, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>los mejillones al vino blanco y puerros, riquí...</td>\n",
       "      <td>[los, mejillones, al, vino, blanco, y, puerros...</td>\n",
       "      <td>[O, B-CONCEPT, O, B-CONCEPT, I-CONCEPT, O, B-C...</td>\n",
       "      <td>[0, 3, 0, 3, 4, 0, 3, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              document  \\\n",
       "486      la atención maravillosa, imposible pedir más.   \n",
       "22   salchicha alemana muy grande y el secreto muy ...   \n",
       "464  los mejillones al vino blanco y puerros, riquí...   \n",
       "\n",
       "                                                tokens  \\\n",
       "486  [la, atención, maravillosa, ,, imposible, pedi...   \n",
       "22   [salchicha, alemana, muy, grande, y, el, secre...   \n",
       "464  [los, mejillones, al, vino, blanco, y, puerros...   \n",
       "\n",
       "                                                labels  \\\n",
       "486                  [O, O, B-MODIFIER, O, O, O, O, O]   \n",
       "22   [B-CONCEPT, O, B-MODIFIER, I-MODIFIER, O, O, B...   \n",
       "464  [O, B-CONCEPT, O, B-CONCEPT, I-CONCEPT, O, B-C...   \n",
       "\n",
       "                           ner_tags  \n",
       "486        [0, 0, 1, 0, 0, 0, 0, 0]  \n",
       "22   [3, 0, 1, 2, 0, 0, 3, 1, 2, 0]  \n",
       "464  [0, 3, 0, 3, 4, 0, 3, 0, 1, 0]  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [\"O\",\"B-MODIFIER\",\"I-MODIFIER\",\"B-CONCEPT\",\"I-CONCEPT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint  = \"PlanTL-GOB-ES/roberta-large-bne-sqac\"\n",
    "tokenizer         = RobertaTokenizerFast.from_pretrained(model_checkpoint,add_prefix_space=True) \n",
    "#model_checkpoint = 'mrm8488/TinyBERT-spanish-uncased-finetuned-ner'\n",
    "#tokenizer        = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'Ġtanto',\n",
       " 'Ġla',\n",
       " 'ĠpanaderÃŃa',\n",
       " 'Ġcomo',\n",
       " 'Ġel',\n",
       " 'Ġrestaurante',\n",
       " 'Ġdan',\n",
       " 'Ġun',\n",
       " 'Ġservicio',\n",
       " 'Ġmuy',\n",
       " 'Ġbueno',\n",
       " 'Ġ',\n",
       " '.',\n",
       " '</s>']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(ds_reviews[\"train\"][1][\"tokens\"], is_split_into_words=True)\n",
    "inputs.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 1118, 332, 41367, 469, 344, 5684, 3592, 355, 1416, 728, 3333, 275, 68, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 3, 0, 0, 3, 1, 2, 0]\n",
      "[-100, 0, 0, 0, 0, 0, 3, 0, 0, 3, 1, 2, 0, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "labels = ds_reviews[\"train\"][1][\"ner_tags\"]\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels)\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datos/luis/.cache/datasets/pandas/default-b6c22ba7c928d56f/0.0.0/6197c1e855b639d75a767140856841a562b7a71d129104973fe1962594877ade/cache-c44e146872687141.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "830b4cdf7fd54e5f9e1d38f377d7cbb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datos/luis/.cache/datasets/pandas/default-b6c22ba7c928d56f/0.0.0/6197c1e855b639d75a767140856841a562b7a71d129104973fe1962594877ade/cache-40fcf00a91ec5c02.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = ds_reviews.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns= ds_reviews[\"train\"].column_names,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 32])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(5)])\n",
    "batch[\"labels\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {str(i): label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at PlanTL-GOB-ES/roberta-large-bne-sqac were not used when initializing RobertaForTokenClassification: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at PlanTL-GOB-ES/roberta-large-bne-sqac and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForTokenClassification.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config._num_labels = 5\n",
    "model.config.id2label    = id2label\n",
    "model.config.label2id    = label2id\n",
    "new_conf = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at PlanTL-GOB-ES/roberta-large-bne-sqac were not used when initializing RobertaForTokenClassification: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at PlanTL-GOB-ES/roberta-large-bne-sqac and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_new = RobertaForTokenClassification.from_pretrained(model_checkpoint,config=new_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    \"ner_roberta\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model                  = model_new,\n",
    "    args                   = args,\n",
    "    train_dataset          = tokenized_datasets[\"train\"],\n",
    "    eval_dataset           = tokenized_datasets[\"validation\"],\n",
    "    data_collator          = data_collator,\n",
    "    compute_metrics        = compute_metrics,\n",
    "    tokenizer              = tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 2600\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1625\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1625' max='1625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1625/1625 04:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.132171</td>\n",
       "      <td>0.838548</td>\n",
       "      <td>0.899710</td>\n",
       "      <td>0.868053</td>\n",
       "      <td>0.953966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.128800</td>\n",
       "      <td>0.111602</td>\n",
       "      <td>0.905622</td>\n",
       "      <td>0.914629</td>\n",
       "      <td>0.910103</td>\n",
       "      <td>0.966233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.128800</td>\n",
       "      <td>0.124722</td>\n",
       "      <td>0.901116</td>\n",
       "      <td>0.936593</td>\n",
       "      <td>0.918512</td>\n",
       "      <td>0.971125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.027500</td>\n",
       "      <td>0.132445</td>\n",
       "      <td>0.908391</td>\n",
       "      <td>0.928719</td>\n",
       "      <td>0.918443</td>\n",
       "      <td>0.970574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.138404</td>\n",
       "      <td>0.907958</td>\n",
       "      <td>0.936179</td>\n",
       "      <td>0.921853</td>\n",
       "      <td>0.971883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 763\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ner_roberta/checkpoint-325\n",
      "Configuration saved in ner_roberta/checkpoint-325/config.json\n",
      "Model weights saved in ner_roberta/checkpoint-325/pytorch_model.bin\n",
      "tokenizer config file saved in ner_roberta/checkpoint-325/tokenizer_config.json\n",
      "Special tokens file saved in ner_roberta/checkpoint-325/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 763\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ner_roberta/checkpoint-650\n",
      "Configuration saved in ner_roberta/checkpoint-650/config.json\n",
      "Model weights saved in ner_roberta/checkpoint-650/pytorch_model.bin\n",
      "tokenizer config file saved in ner_roberta/checkpoint-650/tokenizer_config.json\n",
      "Special tokens file saved in ner_roberta/checkpoint-650/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 763\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ner_roberta/checkpoint-975\n",
      "Configuration saved in ner_roberta/checkpoint-975/config.json\n",
      "Model weights saved in ner_roberta/checkpoint-975/pytorch_model.bin\n",
      "tokenizer config file saved in ner_roberta/checkpoint-975/tokenizer_config.json\n",
      "Special tokens file saved in ner_roberta/checkpoint-975/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 763\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ner_roberta/checkpoint-1300\n",
      "Configuration saved in ner_roberta/checkpoint-1300/config.json\n",
      "Model weights saved in ner_roberta/checkpoint-1300/pytorch_model.bin\n",
      "tokenizer config file saved in ner_roberta/checkpoint-1300/tokenizer_config.json\n",
      "Special tokens file saved in ner_roberta/checkpoint-1300/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 763\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ner_roberta/checkpoint-1625\n",
      "Configuration saved in ner_roberta/checkpoint-1625/config.json\n",
      "Model weights saved in ner_roberta/checkpoint-1625/pytorch_model.bin\n",
      "tokenizer config file saved in ner_roberta/checkpoint-1625/tokenizer_config.json\n",
      "Special tokens file saved in ner_roberta/checkpoint-1625/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1625, training_loss=0.04958951918895428, metrics={'train_runtime': 270.5596, 'train_samples_per_second': 48.049, 'train_steps_per_second': 6.006, 'total_flos': 1020282074889120.0, 'train_loss': 0.04958951918895428, 'epoch': 5.0})"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ner_roberta/checkpoint-975/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"ner_roberta/checkpoint-975\",\n",
      "  \"_num_labels\": 5,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-MODIFIER\",\n",
      "    \"2\": \"I-MODIFIER\",\n",
      "    \"3\": \"B-CONCEPT\",\n",
      "    \"4\": \"I-CONCEPT\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"B-CONCEPT\": \"3\",\n",
      "    \"B-MODIFIER\": \"1\",\n",
      "    \"I-CONCEPT\": \"4\",\n",
      "    \"I-MODIFIER\": \"2\",\n",
      "    \"O\": \"0\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50262\n",
      "}\n",
      "\n",
      "loading configuration file ner_roberta/checkpoint-975/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"ner_roberta/checkpoint-975\",\n",
      "  \"_num_labels\": 5,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-MODIFIER\",\n",
      "    \"2\": \"I-MODIFIER\",\n",
      "    \"3\": \"B-CONCEPT\",\n",
      "    \"4\": \"I-CONCEPT\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"B-CONCEPT\": \"3\",\n",
      "    \"B-MODIFIER\": \"1\",\n",
      "    \"I-CONCEPT\": \"4\",\n",
      "    \"I-MODIFIER\": \"2\",\n",
      "    \"O\": \"0\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50262\n",
      "}\n",
      "\n",
      "loading weights file ner_roberta/checkpoint-975/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForTokenClassification.\n",
      "\n",
      "All the weights of RobertaForTokenClassification were initialized from the model checkpoint at ner_roberta/checkpoint-975.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForTokenClassification for predictions without further training.\n",
      "Didn't find file ner_roberta/checkpoint-975/added_tokens.json. We won't load it.\n",
      "loading file ner_roberta/checkpoint-975/vocab.json\n",
      "loading file ner_roberta/checkpoint-975/merges.txt\n",
      "loading file ner_roberta/checkpoint-975/tokenizer.json\n",
      "loading file None\n",
      "loading file ner_roberta/checkpoint-975/special_tokens_map.json\n",
      "loading file ner_roberta/checkpoint-975/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "# Replace this with your own checkpoint\n",
    "model_checkpoint = \"ner_roberta/checkpoint-975\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "por cierto, el café, muy bueno.\n"
     ]
    }
   ],
   "source": [
    "review = df_test.iloc[9].document\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-CONCEPT',\n",
       "  'score': 0.9999236,\n",
       "  'index': 5,\n",
       "  'word': 'ĠcafÃ©',\n",
       "  'start': 15,\n",
       "  'end': 19},\n",
       " {'entity': 'B-MODIFIER',\n",
       "  'score': 0.9991999,\n",
       "  'index': 7,\n",
       "  'word': 'Ġmuy',\n",
       "  'start': 21,\n",
       "  'end': 24},\n",
       " {'entity': 'I-MODIFIER',\n",
       "  'score': 0.9997008,\n",
       "  'index': 8,\n",
       "  'word': 'Ġbueno',\n",
       "  'start': 25,\n",
       "  'end': 30}]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_classifier(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'café'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review[15:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'muy bueno'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review[21:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e11ffe9bd754d42c2fe650a6e02d2a256e04b0b447510aec6eb3ae5921c14a6d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('relation_extraction')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
